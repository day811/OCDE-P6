{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A réaliser : \n",
    "- Une analyse descriptive des données, y compris une explication du sens des colonnes gardées, des arguments derrière la suppression de lignes ou de colonnes, des statistiques descriptives et des visualisations pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bc_after_eda = pd.read_csv(\"data/bc_after_eda.csv\", index_col='Unnamed: 0')\n",
    "bc_after_eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV, \n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.metrics import r2_score, mean_absolute_error , root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Preprocess\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler,FunctionTransformer\n",
    "\n",
    "#Modèles\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A réaliser : Enrichir le jeu de données actuel avec de nouvelles features issues de celles existantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des features pour la modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A réaliser :\n",
    "* Si ce n'est pas déjà fait, supprimer toutes les colonnes peu pertinentes pour la modélisation.\n",
    "* Tracer la distribution de la cible pour vous familiariser avec l'ordre de grandeur. En cas d'outliers, mettez en place une démarche pour les supprimer.\n",
    "* Débarrassez-vous des features redondantes en utilisant une matrice de corrélation.\n",
    "* Réalisez différents graphiques pour comprendre le lien entre vos features et la target (boxplots, scatterplots, pairplot si votre nombre de features numériques n'est pas très élevé).\n",
    "*  Séparez votre jeu de données en un Pandas DataFrame X (ensemble de feautures) et Pandas Series y (votre target).\n",
    "* Si vous avez des features catégorielles, il faut les encoder pour que votre modèle fonctionne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modes énergétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_after_eda['UseGas'] = (bc_after_eda['NaturalGas(kBtu)'].notna()) & (bc_after_eda['NaturalGas(kBtu)'] != 0)\n",
    "bc_after_eda['UseSteam'] = (bc_after_eda['SteamUse(kBtu)'].notna()) & (bc_after_eda['SteamUse(kBtu)'] != 0)\n",
    "bc_after_eda['UseElectricity'] = (bc_after_eda['Electricity(kBtu)'].notna()) & (bc_after_eda['Electricity(kBtu)'] != 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distance du centre ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Entrées: Series -> conversion séparée\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    R = 3958.8\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_lat, seattle_lon = 47.6085965, -122.5049456\n",
    "\n",
    "bc_after_eda['CityDistance'] = haversine(\n",
    "    bc_after_eda['Latitude'], bc_after_eda['Longitude'],\n",
    "    seattle_lat, seattle_lon\n",
    ").round(2)\n",
    "bc_after_eda['CityDistance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_after_eda['MultipleUseType'] = bc_after_eda['ListOfAllPropertyUseTypes'].str.count('s')+1\n",
    "bc_after_eda['MultipleUseType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_after_eda.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_values = ['3LargestGFA', 'FirstUseType','SecondLargestPropertyUseType', 'MultipleUseType', 'UseSteam', 'UseElectricity', 'UseGas',\n",
    "       'NumberofFloors', 'NumberofBuildings', 'CityDistance', 'Neighborhood','YearBuilt']\n",
    "X = bc_after_eda[predict_values]\n",
    "target = 'SiteEnergyUse(kBtu)'\n",
    "y = bc_after_eda[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index X_train avant pipeline :\", X_train.index.equals(y_train.index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalisation des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# ÉTAPE 1: PREPROCESSING PERSONNALISÉ \n",
    "# ========================\n",
    "\n",
    "def fix_floors_and_discretize(df):\n",
    "    \"\"\"Fonction qui fait tout votre preprocessing d'un coup\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Corriger NumberofFloors\n",
    "    mask = (df['NumberofFloors'] < 1)\n",
    "    OneBuildingMeanFloor = df[df['NumberofBuildings']==1][\"NumberofFloors\"].mean()\n",
    "    OneBuildingMeanFloor = int(OneBuildingMeanFloor.round(0))\n",
    "    df.loc[mask,'NumberofFloors'] = OneBuildingMeanFloor\n",
    "    \n",
    "    # 2. AgeProperty et AgeCategory\n",
    "    df['AgeProperty'] = 2016 - df['YearBuilt']\n",
    "    df['AgeCategory'] = pd.cut(df['AgeProperty'],\n",
    "                               bins=[0, 20, 40, 70, df['AgeProperty'].max()],\n",
    "                               labels=['Neuf', 'Récent', 'Ancien', 'Historique'],\n",
    "                               include_lowest=True)\n",
    "    \n",
    "    # 3. EnergyEra\n",
    "    df['EnergyEra'] = pd.cut(df['YearBuilt'],\n",
    "                             bins=[1900, 1980, 2000, 2016],\n",
    "                             labels=['Pre-Crisis', 'Modern', 'Contemporary'],\n",
    "                             include_lowest=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 4. PropertySize (3LargestGFA) – quantiles auto sur train\n",
    "    if not hasattr(fix_floors_and_discretize, 'size_bins'):\n",
    "        _, fix_floors_and_discretize.size_bins = pd.qcut(\n",
    "            df['3LargestGFA'], q=4, retbins=True, duplicates='drop'\n",
    "        )\n",
    "    df['PropertySize'] = pd.cut(df['3LargestGFA'],\n",
    "                                bins=fix_floors_and_discretize.size_bins,\n",
    "                                labels=['Small', 'Mid', 'Large', 'XLarge'],\n",
    "                                include_lowest=True)\n",
    "    \n",
    "    # 5. HeightCategory (NumberofFloors) – quantiles auto sur train\n",
    "    if not hasattr(fix_floors_and_discretize, 'floor_bins'):\n",
    "        _, fix_floors_and_discretize.floor_bins = pd.qcut(\n",
    "            df['NumberofFloors'], q=3, retbins=True, duplicates='drop'\n",
    "        )\n",
    "    df['HeightCategory'] = pd.cut(df['NumberofFloors'],\n",
    "                                  bins=fix_floors_and_discretize.floor_bins,\n",
    "                                  labels=['Low', 'Mid', 'High'],\n",
    "                                  include_lowest=True)\n",
    "    return df\n",
    "\n",
    "# ========================\n",
    "# ÉTAPE 2: PIPELINE COMPLET\n",
    "# ========================\n",
    "\n",
    "# Colonnes après votre preprocessing\n",
    "categorical_features = ['FirstUseType', 'SecondLargestPropertyUseType', 'PropertySize',\n",
    "                       'Neighborhood','AgeCategory','EnergyEra','HeightCategory']  # Ajoutez vos autres catégories ici\n",
    "\n",
    "numerical_features = ['3LargestGFA',\n",
    "                     'CityDistance', 'MultipleUseType', 'NumberofFloors','NumberofBuildings']\n",
    "\n",
    "# Pipeline complet\n",
    "full_pipeline = Pipeline([\n",
    "    # Étape 1: Preprocessing personnalisé\n",
    "    ('preprocessing', FunctionTransformer(fix_floors_and_discretize, validate=False)),\n",
    "    \n",
    "    # Étape 2: Encodage + Normalisation\n",
    "    ('encoder', ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), \n",
    "         categorical_features),\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ], remainder='passthrough'))\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# ÉTAPE 3: APPLICATION \n",
    "# ========================\n",
    "\n",
    "# Fit sur train et transform train/test\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train)\n",
    "X_test_transformed = full_pipeline.transform(X_test)\n",
    "\n",
    "# ========================\n",
    "# ÉTAPE 4: CRÉER VOS DataFrames _final\n",
    "# ========================\n",
    "\n",
    "# Récupérer les noms des colonnes\n",
    "onehot = full_pipeline.named_steps['encoder'].named_transformers_['cat']\n",
    "onehot_names = onehot.get_feature_names_out(categorical_features)\n",
    "num_names = [f\"scaled_{col}\" for col in numerical_features]\n",
    "\n",
    "# Colonnes restantes (passthrough)\n",
    "all_cols_after_preprocessing = fix_floors_and_discretize(X_train).columns\n",
    "remaining_cols = [col for col in all_cols_after_preprocessing \n",
    "                 if col not in categorical_features + numerical_features]\n",
    "\n",
    "# Noms finaux\n",
    "final_feature_names = list(onehot_names) + num_names + remaining_cols\n",
    "\n",
    "# Vos DataFrames finaux\n",
    "X_train_final = pd.DataFrame(X_train_transformed, columns=final_feature_names,index=X_train.index )\n",
    "X_test_final = pd.DataFrame(X_test_transformed, columns=final_feature_names,index=X_test.index)\n",
    "\n",
    "# Conversion en numérique\n",
    "for col in X_train_final.columns:\n",
    "    X_train_final[col] = pd.to_numeric(X_train_final[col], errors='coerce')\n",
    "    X_test_final[col] = pd.to_numeric(X_test_final[col], errors='coerce')\n",
    "\n",
    "X_train_final.drop(columns=['YearBuilt','AgeProperty'], inplace=True)\n",
    "X_test_final.drop(columns=['YearBuilt','AgeProperty'], inplace=True)\n",
    "\n",
    "print(f\"✅ Pipeline terminé!\")\n",
    "print(f\"Shape finale: Train {X_train_final.shape}, Test {X_test_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index X_train_final après pipeline :\", X_train_final.index.equals(y_train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE COMPARAISON DES MODELES\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "models = {\n",
    "    'DummyRegressor': DummyRegressor(strategy='mean'),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'SVR': SVR(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "scoring = ['neg_root_mean_squared_error','r2','neg_mean_absolute_error']\n",
    "cv_results = {}\n",
    "\n",
    "print(\"=== COMPARAISON MODÈLES AVEC PIPELINE COMPLET ===\")\n",
    "for name, model in models.items():\n",
    "    # Pipeline complet + modèle final\n",
    "    full_estimator = Pipeline([\n",
    "        ('preprocess', full_pipeline.named_steps['preprocessing']),\n",
    "        ('encode_scale', full_pipeline.named_steps['encoder']),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    scores = cross_validate(full_estimator, X_train, y_train, \n",
    "                            cv=5, scoring=scoring, n_jobs=-1)\n",
    "    cv_results[name] = {\n",
    "        'RMSE': -scores['test_neg_root_mean_squared_error'].mean(),\n",
    "        'R2': scores['test_r2'].mean(),\n",
    "        'MAE': -scores['test_neg_mean_absolute_error'].mean()\n",
    "    }\n",
    "    print(f\"{name} → R²: {cv_results[name]['R2']:.3f}, RMSE: {cv_results[name]['RMSE']:.0f}, MAE: {cv_results[name]['MAE']:.0f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice= \"MAE\"\n",
    "if choice == \"R2\":\n",
    "    best_model_name = max(cv_results.keys(), key=lambda x: cv_results[x][choice])\n",
    "else:\n",
    "    best_model_name = min(cv_results.keys(), key=lambda x: cv_results[x][choice])\n",
    "print(f\"\\n🏆 Meilleur modèle selon {choice}: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation et interprétation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A réaliser :\n",
    "* Reprennez le meilleur algorithme que vous avez sécurisé via l'étape précédente, et réalisez une GridSearch de petite taille sur au moins 3 hyperparamètres.\n",
    "* Si le meilleur modèle fait partie de la famille des modèles à arbres (RandomForest, GradientBoosting) alors utilisez la fonctionnalité feature importance pour identifier les features les plus impactantes sur la performance du modèle. Sinon, utilisez la méthode Permutation Importance de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons best_model_name défini précédemment\n",
    "#best_model_name = \"RandomForest\"\n",
    "print(f\"=== OPTIMISATION de {best_model_name} ===\")\n",
    "\n",
    "if best_model_name == 'RandomForest':\n",
    "    estimator = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "elif best_model_name == 'GradientBoosting':\n",
    "    estimator = GradientBoostingRegressor(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.1, 0.05, 0.01],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "elif best_model_name == 'LinearRegression':\n",
    "    estimator = LinearRegression()\n",
    "    param_grid = {\n",
    "        'fit_intercept': [True, False],\n",
    "        'positive': [False, True]\n",
    "    }\n",
    "\n",
    "elif best_model_name == 'SVR':\n",
    "    estimator = SVR()\n",
    "    param_grid = {\n",
    "        'kernel': ['rbf', 'linear', 'poly'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Modèle non supporté : {best_model_name}\")\n",
    "\n",
    "# Lancer GridSearchCV\n",
    "gs = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Meilleurs paramètres :\", gs.best_params_)\n",
    "print(\"Meilleur score CV (RMSE) :\", -gs.best_score_)\n",
    "\n",
    "# Évaluation finale\n",
    "final_model = gs.best_estimator_\n",
    "y_pred = final_model.predict(X_test_final)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE FINALE sur TEST ===\")\n",
    "print(f\"RMSE: {root_mean_squared_error(y_test, y_pred):.0f}\")\n",
    "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.0f}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== OPTIMISATION de LinearRegression ===  \n",
    "Fitting 5 folds for each of 4 candidates, totalling 20 fits  \n",
    "Meilleurs paramètres : {'fit_intercept': False, 'positive': True}  \n",
    "Meilleur score CV (RMSE) : 15216932.25998593  \n",
    "  \n",
    "=== PERFORMANCE FINALE sur TEST ===  \n",
    "RMSE: 9128882  \n",
    "R²: 0.7541  \n",
    "MAE: 4289626  \n",
    "MAPE: 1.4691  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "feature_names = X_train_final.columns\n",
    "\n",
    "# Affichage du top 15 features les plus importantes\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "print(\"Top 30 features by importance:\")\n",
    "for idx in sorted_idx[:30]:\n",
    "    print(f\"- {feature_names[idx]}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde BentoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "bentoml.sklearn.save_model(\n",
    "    name=\"building_energy_rf_pipeline\",\n",
    "    model=full_pipeline,\n",
    "    signatures={\"predict\": {\"batchable\": True}},\n",
    "    metadata={\n",
    "        \"author\": \"day811\",\n",
    "        \"feature_names\": list(feature_names)\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(X_train_final.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocde-p6-qTWNOnCB-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
